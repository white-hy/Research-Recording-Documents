# SVM使用经验

## Kernel RBF

`C`参数衡量了训练样本的误分类概率，注意到C在起作用是
$$
min_{w,b,\xi}\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i\\
s.t.y_i(wx_i+b)\geq1-\xi_i,i=1,2,...,N\\
\xi_i\geq0,i=1,2,...,N
$$
C比较小就相当于某个$\xi_i$可以比较大，也就是某个样本可以被误分类，C比较大的时候就可以禁止误分类，当数据噪声比较大的时候可以把C设置得比较大。

`gamma`参数在`RBF`Kenel中起的用处是$exp(-\gamma||x-x'||^2)$, 这也就是说对于此时`gamma`扮演的角色是一个影响半径，决定了一个样本的影响。

注意到支持向量机的对偶问题（最大最小拉格朗日函数）是：
$$
min_{\alpha}\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i*x_j)-\sum_{i=1}^N\alpha_i\\
s.t.\sum_{i=1}^N\alpha_iy_i=0\\
0\leq \alpha_i \leq C,i=1,2,...,N
$$
相应的解是
$$
suppose\ \alpha^*=(\alpha_1^*,...,\alpha_N^*)^T是对偶问题的解，若存在某一分量\alpha_j^*,\\0<\alpha_j^*<C,那么原始问题的解w^*,b^*可以如下求得\\
w^*=\sum_{i=1}^N\alpha_i^*y_ix_i\\
b^*=y_j-\sum_{i=1}^Ny_i\alpha_i^*<x_i,x_j>
$$
那么此时代入$ker(x,y)$函数，我们就有：

$f(x)=sign(\sum_{i=1}^N\alpha_i^*y_iK(x,x_i)+b^*)$这样的公式，代入$K(x,x_i)=exp(-\gamma||x-x_i||^2)$，此时$\gamma$超参数就表示每一个点$x_i$对于当前数据分类的影响，如果$\gamma$太大，那么一旦x与任意一个样本点$x_i$离的有点远，样本点对于x的影响就会几近于0. 如果$\gamma$太小，那么x与任意一个样本点离的有点近的时候，它们的距离就会被缩的特别小，也就是近的样本点对于x的影响被放大了。

这就是[如何选取超参数](http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py)这篇文章中所述的，当`gamma`很小的时候，模型就非常容易被好多的支持向量一起影响（考虑exp(-x）的图像，此时模型就很难捕捉数据的局部形状，因为受其它数据的影响实在是太多了），无法去捕捉数据的形状，因此导致的结果就是模型会很相似于一个线性的模型，它分割的结果就是按密度中心把两个数据集分割开，也就是gamma越小模型越"平滑",e而C值会让模型分错的代价变大，因此模型可能就会变的更复杂。当`gamma`很大的时候，模型就会被限制在局部，离哪个点近分为哪个，就是一个kNN，它是非常过拟合的（显然每一个数据此时都会被正确分类，因为局部影响增强了），而再大的C也无法阻止过拟合（C越大分的越准，而gamma越大也会分的很准）

总结就是：

gamma越小越不过拟合，gamma越大越过拟合

C控制了误分类的概率，C越小误分类概率越大，C越大越容易过拟合。

一般采用`SVM`的`RBF` kernel的时候，可以先选C，用"auto"来确定gamma，在选取C比较好后再对gamma进行一下搜索,C设的小一点对于gamma而言环境就更宽松一些，因为此时C控制的loss就小一些。



## Kernel poly

$$
polynomial=(\gamma<x,x'>+r)^d,d\ \text{is degree , r is coef0},\\ \gamma \ \text{is the "ratio"}
$$

因此假设$K(x,y)=(\gamma(x_1y_1+...+x_ny_n)+r)^d$

这个意思也就是将x映射到某个H空间，p次多项式分类器？

$f(x)=sign(\sum_{i=1}^Na_i^*y_i(\gamma <x_i,x>+coef0)^p+b)$这样一个公式

也就是说此时x和支持向量的方向越相似，那么权重就会越大。而此时应该与kNN分类器相似吧，这里是距离相似

与论文中的kNN对比:

$w_c=\sum_{i\in N_k}exp(<x_i,x>/\tau)*1(c_i=c)$ 然后找到最大的分类权重,$\tau=0.07$，此时exp在这里是n次多项式了，也就是exp是泰勒无穷阶展开，这自然比一阶或二阶要好一些,但是确也不一定，将核展开：
$$
K(x,y)=(\gamma<x,y>+r)^d=(\gamma<x,y>)^d+d(\gamma<x,y>)^{d-1}r+\\ \sum_{k=2}^{d-1} C_{d}^k(\gamma<x,y>)^kr^{d-k}+r^d
$$
也就是关于$<x,y>$ 的一个d次多项式，此时$\gamma$也是衡量局部影响力的。假设$\gamma$非常大，那么最后结果就会被局部所捕捉，也就是会退化到只有支持向量的kNN，如果$\gamma$很小，那么同样对一个点的预测就受好多点的影响，这也就意味着模型无法捕捉局部形状。结合我们计算discriminator的方法，那么我们可以认为适当的$\gamma$更加灵活。而对于$r$参数而言呢, $r$参数是用于控制多项式中各项的比例问题的，r取得越小模型越会限制低次，也就是低次项对于模型的修正变得不那么重要了，也就是模型会欠拟合。因此随着r增大，模型拟合能力增加直到0.5的时候，超过了这个之后模型过拟合能力减少，因此r就是这个意思。

关于d参数呢？

d=1的时候就是单纯用支持向量做的kNN（类似），此时准确率基本都是79%的样子，说明我们的这个特征和分类还是很靠谱的。

随着d增大，相当于我们用更复杂的曲线（映射）来描摹整个函数，d越大就越可能过拟合。我们可以看到在测试集上面随着d越来越大，测试集的精度一般是先增加再减少，等d很大的时候就非常少了，而在训练集上的精度则一直在增加，也就是可以良好拟合训练集了。

因此d小是欠拟合，d越大越过拟合，而coef0参数则调整的是在取定d后，次数小于d的那些项的平衡。

注意在我们这个问题中d=2的时候就把训练集和测试集很好分开了，所以这似乎说明poly更适合我们的问题。

## Kernel Sigmoid

$$
sigmoid\ kernel=tanh(\gamma<x,x'>+coef0)\\
tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\\
K(x,y)=\frac{e^{\gamma<x,y>+coef0}-e^{-\gamma<x,y>-coef0}}{e^{\gamma<x,y>+coef0}+e^{-\gamma<x,y>-coef0}}
$$

 $<x,y>\rightarrow1,K(x,y)\rightarrow1,<x,y>\rightarrow-1,K(x,y)\rightarrow-1$.

用tanh来分类相当于考虑了两种支持向量，即远离不像的，接近像的。结合以前次的经验，我们对这几个参数进行如下的预测：

$f(x)=sign(\sum_{i=1}^N\alpha_i^*y_iK(x,x_i)+b^*)$

$\gamma$非常巨大，那么这样就会让相似的更相似，或者说此时会导致两级分化，如果$<x,x_i>>0$那么$K(x,x_i)$就会向1靠拢，反之向-1靠拢。此时就相当于投票到离支持向量多的那一边。

$\gamma$比较小，$K(x,x_i)$都会比较靠近于0，中间部分被过滤了，只有那些影响特别大的才能捕捉信息，如果极其小那么所有信息都无法捕捉，欠拟合。

因此$\gamma$由小到大就是欠拟合到过拟合的一个过程。

coef0呢？它似乎是起到一个限制的作用，我觉得在这里设置为0比较好。

动手试试吧：

1.sigmoid似乎并不适合我们的这个问题，它对于训练集上的分类精度都达不到我们的要求，在测试集上更是一塌糊涂（大概80%的样子）

2.sigmoid中对于$\gamma$非常敏感，$\gamma$一般只能取到[0,1]z之间，小于大于都会崩

3.coef系数可能就是一个偏置系数，可以在[-1,1]之间进行微调以增加准确度

## Kernel kNN

这次我们用kNN的kernel作为实验kernel，也就是说用：

$K(x,y)=exp(\gamma<x,y>+coef0)$  来作为核，首先这个矩阵是实对称矩阵，对角元素最大。所以一般。。应该可能是正定的吧？

因为对角大于0，而且对角严格占优，因此此时是正定或半正定的，因此此时为正定核函数。

此时gamma取4的时候我觉得效果非常不错，同时coef取0，训练集的分类准确率为1.0，而测试集为0.81，复现了kNN的分类手法。













