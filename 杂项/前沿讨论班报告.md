# 前沿讨论班报告

**冯浩哲**

**3140104019**

**统计1401**

+++++

[TOC]

## 报告概述

在本次讨论班中，我进行了2次报告。

在第一次报告中我对*Riemann*曲面的拓扑结构进行了时长为2节课的介绍。这次报告我主要是用了书籍[An Introduction to Riemann Surfaces, Algebraic Curves and Moduli Spaces](https://link.springer.com/book/10.1007/BFb0113492)上**Chapter1,2**的一些内容，主要涉及到**Chapter2.1**的基本群。在此报告中，我将对第一次报告的内容进行概述与总结。

在第二次报告中，我进行了题目为*Statistic, Machine Learning & Deep Learning : An introduction review*的一次报告，时长为三节课。第二次报告的内容参考了一些文献，但是大体为我自己所撰写的。在此报告中，我也将对这些内容进行概述总结。



## *Riemann*曲面的拓扑结构之基本群

报告主要叙述*Riemann*曲面的第一拓扑特征，即研究在曲面上的环(*Loop*)的集合。主要的研究手法是对这个集合进行群性质的定义，然后通过研究这个群来导出Riemann面的拓扑特征。

研究环的基础是流型，流型$M$可以简述为每一个局部都与$R^n$空间相等价的拓扑空间，它具有如下几个性质：

1. 具有开覆盖${U_i}$使得$\cup U_{i}=M$.且对$\forall U_i$,存在连续双射$\phi_i,s.t.\ \phi_{i}:U_{i}\rightarrow W_{i}\subset R^n$,同时$W_i$为开集
2. 流型$M$是可分拓扑空间，即假设存在$\alpha,\beta \in M,\alpha\neq\beta$,那么存在$V_\alpha,V_\beta,s.t.\ \alpha\in V_\alpha,\beta\in V_\beta,V_\alpha\cap V_\beta\neq\emptyset$
3. 流型$M$是准紧的，也就是说$M$的任何一个开覆盖都具有局部有限的加细

然后我们可以研究环。我们通过轨迹(*Path*)的定义来导出*Loop*。假设我们有一个流型$M$，有$M$上固定的两个点$p,q$，同时有R上的一个闭区间$I=[0,1]$，那么轨迹就是一个$I\rightarrow M$的连续映射:$\alpha:I\rightarrow M$,满足$\alpha(0)=p,\alpha(1)=q$。我们将环定义为$p=q$的轨迹。

我们的目的是研究曲面上所有环的集合，主要研究手法是采用抽象代数的思想进行分类，而分类则需要等价关系，因此我们需要构建一个叫同伦(*homotopy*)的等价关系，即如果两个环$\alpha,\beta$等价，那么他们之间有如此定义的一种同伦关系:

存在一个二元，且对于每一元都是连续的映射
$$
h:[0,1]\times[0,1]\rightarrow M,s.t.\ h(t,0)=\alpha(t),h(t,1)=\beta(t),\forall t\in[0,1]\\
h(0,u)=p,h(1,u)=q,\forall u\in [0,1]
$$


可以简单地证明，同伦关系下对给定的一组点，顺序不一样的路径依然是等价的。

注意到作为群来研究,我们还需要定义一个二元运算。假设$\alpha(0)=p,\alpha(1)=q,\beta(0)=q,\beta(1)=v$，那么我们可以定义$\gamma=\alpha\beta$,满足$\gamma(t)=\alpha(2t),if\ t\in [0,1/2],or\  \gamma(t)=\beta(2t-1),if\ t\in[1/2,1]$

定义逆元即$\alpha^{-1}(t)=\alpha(1-t)$

在这些运算的基础上我们可以研究环，我们通过定义基本群研究环。

定义流型$M$关于$p$ 的基本群为$\pi(M,p)={所有在p点的同伦类loop所组成的集合}$，其加法与逆元已经如上定义了。注意这个p点不具有特殊性(**命题2.2**)。

在这个定义的的框架下，我们可以进行一些分类，比如存在一种流型$M$，使得$\pi(M)={1}$，也就是说所有的环都等价于单位元，或者说所有的环都可以缩小到一点。它的直观代表是球，而反例是面包圈，面包圈的环不能缩小到一点。

## Statistic, Machine Learning & Deep Learning : An introduction review

报告主要介绍了统计学与机器学习的关系，以及机器学习与深度学习之间的关系。我将在里面以链接的方式给出所引用的论文。

### 机器学习和统计学的关系

这里我主要引用[Statistical Modeling: The Two Cultures](https://projecteuclid.org/euclid.ss/1009213726)文献中的一些观点加上一些自己的理解与解释进行展开。

#### 统计学历史简溯

应用统计学一般有2个主流派别，一个是数据模型(data model)，另一个是算法模型(Algorithm model)。

Data model的主要方法是回归，包括线性回归与非线性回归。线性回归的基本模型是这样的:
$$
y=b_0+\sum_{i=1}^{M}b_mx_m+\epsilon\\
\epsilon\ s.t.\ Gauss-Markov Assumption\\
\epsilon \sim N(0,\delta^2)
$$
根据该模型，我们建立估计步骤如下：

1. 采用极大似然估计/最小二乘估计对参数进行估计

2. 依据假设对系数进行显著性水平的F检验，最后对残差进行正态性检验。

   如果残差的正态性检验通过了5%的置信度水平假设，那么模型就符合Gauss-Markov假设，模型关系在统计学意义上成立。

但是这种做法有一个致命的缺陷，就是残差检验只是对残差项的分布进行检验，但是无法检验我们的数据到底是不是线性模型所生成的。事实上，模型的线性性检验一般是通过拟合度检验$R^2$,如果$R^2$接近于1，一般可以推断模型的拟合能力好，即线性模型是成立的。

拟合优度与残差检验构成了线性模型的基本逻辑基础。但是，这两个检验对于模型中有非线性项是不成立的。如果我们只考虑线性模型，那么一个显著的问题就是，这些检验的精度怎么样？

对于拟合优度检验，有很多研究表明，除非是极端的不fit，拟合度测试才会reject。

对于残差分析而言，William Cleveland，残差分析的发明人之一，承认超过四维的数据，残差分析并不能测试出其真正的拟合程度，好的残差图也并不能说明数据很fit模型。这是因为，线性性是一个很强的假设。一般来说，数据可以和多个模型同时匹配，因此单一的线性模型的难度在于，我们很难构造一个完整的模型，这个完整的模型包含了所有的备选线性模型。

 同时我们以小见大，通过上述线性模型我们可以看出Data model的第一步往往是假设数据的生成过程符合以下分布然后是数学推导和假设检验，对数据进行参数建模，根据模型给出结论。但是，对于数据建模的时候，一个很显然的问题就是，这样得出的结论是根据你设想的模型得出的，而不一定是真正的自然机制。设想的准确度依赖于拟合优度，残差分析等检验，但是这些检验的精度没有那么高，因此如果你的设想是错的，那么结论就是错的。

 一直以来，由于最小二乘法在低维数据上极其少的计算量以及数据获取的困难，线性模型一直是主流模型。同时，由于高维优化问题以及计算能力的不足，预测前的第一步就是降维，避免维度灾难。如果有过多的特征，那么首先要筛选出能表征绝大部分信息量的特征，进行降维。在线性回归，逻辑回归的实践中，一般都建议先进性特征选择。一般认为高维是危险的，统计学大佬Richard Bellman甚至对此有一个名句：*The curse of dimensionality*"，也就是维数灾难的意思。但是最近的研究表明，维度不一定是curse，也有可能是blessing。

同时，随着Computer Science与计算数学研究的进展，我们处理高维优化问题的时候有了许多行之有效的手段。比如，几年前有一篇文献[Identifyingand attacking the saddle point problem in high-dimensional non-convexoptimization](https://arxiv.org/abs/1406.2572)通过统计物理的手段发现了高维优化问题的难点不是我们一直所认为的所谓高维优化问题局部极小值很多，而是鞍点很多。这个发现是一语惊醒梦中人，至此之后许多针对该发现的优化算法提出，它们对于鞍点都有很好的处理方法。对于高维优化算法一个非常好的综述是[An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)，这个Blog分析了高维优化问题的主流算法与特点，并有可视化的过程。

#### 算法模型与机器学习

在这些基础上，统计学在数据模型的基础上又多了一个分支，算法模型。这个模型主要手段是用尽量大的模型容量去拟合现有数据，典型代表为决策树，随机森林，随机条件场，神经网络算法。同时，在这个阶段，模型的主要目的在于预测的准确度而不是预测的精度，因此，我们采用训练集-测试集来对模型进行检验。具体来说，我们用模型对训练集预测与训练集的真值交叉熵作为损失，模型在训练中通过参数迭代来减少损失函数，同时模型的表现能力通过对测试集的预测与真值的残差来进行评估。这种模型的特点就是彻底放弃了对于数据的假设(包括对数据分布，生成情况的假设)，同时放弃了对预测精度的评估(也就是不做置信区间估计)，一切以准确率为主。

在2010年之前，随机森林，决策树，随机条件场都是广泛应用的技术。随机森林与决策树替代了线性模型，在整数数据拟合与多分类任务中取得了很好的效果，而随机条件场在自然语言的词性分析处理中应用广泛。

同时，除了预测任务，数据分类，推荐算法与数据聚类等需求也衍生出一批新的算法，如 **SVM** (支持向量机)， **K-means** 聚类算法，**Active Learning** (主动学习)算法等，这些任务统称为无监督学习任务。Algorithm model与这些无监督学习算法统称为机器学习

### 深度学习与机器学习的关系

#### 从机器学习到深度学习

深度学习的主要工具是机器学习的神经网络算法，而深度学习与机器学习的区别主要在于对于特征的认识。

一般的机器学习算法仍然有一个特点，就是我们仍然需要给出一些特征，而模型从数据中主动学习拟合的是这些特征的权重。以决策树为例，决策树的每一个子节点的二分类标准都是要人为给定的，我们要学习的是这些标准下的具体判定数值，或者这些标准在当前数据中的重要性程度。

深度学习在机器学习的做法上更进一步，深度学习直接从特征本身进行学习，深度学习的核心数学定理是万能近似定理，即一个神经网络如果具有线性输出层和至少一层具有任何一种挤压性质的激活函数(比如sigmoid，ReLU的隐藏层，那么只要给予网络足够数量的隐藏单元，那么它就可以**以任意精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数**。这条定理是Hornik在1990年提出的。

这个定理意味着只要我们构建足够深的神经网络，那么它总能找出我们所需要的特征。这个定理是1990年发现的，但是神经网络在2010年才开始兴起，这是为什么呢？这里我们稍微讲一下神经网络的历史。

#### 神经网络历史简溯

自从1986年Hinton提出[反向传播算法](http://www.cs.toronto.edu/~fritz/absps/pdp8.pdf)神经网络事实上已经解决了参数拟合问题。但是一方面因为单层神经网络并不能解决异或问题，而SVM支持向量积能解决这类问题(也就是单层神经网络并不能将低维难以区分的区域投射到高维区域)，另一方面因为计算机算力不足，多层神经网络训练速度非常慢，无法迅速解决现有问题。简单地来说，就是在这个阶段，机器学习算法比深度学习算法在训练时间与性能上都有压倒性的优势，因此深度学习算法式微，甚至变成了和遗传算法一样在学术界人人喊打的骗局。

改变发生在**2006** 年。随着机器学习算法的成熟，人们不再局限于数字，而将目光转向了自然语言处理，图片与音频处理等方向。如果还记得的话，我们注意到机器学习的一个特征就是人为给定特征空间。但是因为这些新任务的特征空间是人所无法穷尽的，而我们所给出的特征往往并不能完整描述整个数据集的特征，这导致的一个极大的问题就是机器学习结果的False Positive率极大增加，也就是误报率很大。在2006年，Hinton又发了一篇文章[Reducing the dimensionality of data with neural network](http://science.sciencemag.org/content/313/5786/504)指出，多隐层神经网络具有优越的直接学习特征的能力，而它在训练上的复杂度可以通过贪心算法，也就是固定其他层参数来逐层优化进行有效缓解。这篇论文可以说引燃了又一波深度学习的研究，因为它解决了神经网络训练资源的问题，以及提出了神经网络能够主动学习特征的理论。 

同时期，在游戏厂商的推动下，GPU技术逐渐成熟了。如果是学过计算机图形学的同学一定对GPU不陌生。GPU着重于矩阵变换等数值计算方法，通过并行模式大大加快了纯数值计算的速度，而神经网络的优化迭代方法正适合采用GPU的并行算法来解决。GPU的大规模发展，使得高效训练深度神经网络成为可能，事实上现在通过Relu，DropOut以及Batch Normalization等手段，贪心优化法已经逐渐不再使用了。

而同时，经过充分训练的神经网络在目标检测与分类任务上取得了无可比拟的优势。2016年的Faster R-CNN框架下的多分类多实例目标检测在挑战赛上获得了73.2%的正确率，而同期最先进的机器学习算法只有23.3%。(这里的一个对比就是肉眼检测的准确率在70%左右，这基本可以说可以用啦).

### 卷积神经网络，机器视觉与模式识别

因为我是搞图像识别与目标检测的，因此我主要讲讲卷积神经网络。

首先，卷积神经网络的卷积与数学中的卷积有什么区别呢？数学中的卷积公式为
$$
s(t)=\int_{t_1}^{t}x(a)w(t-a)da
$$
这个公式可以直观的表示为如果我们要预测t时刻的一个数据，但是我们有时刻的一系列时间序列数据，这些时间序列数据是有噪声的，为了充分利用这些数据，减少噪声的影响，我们定义一个加权函数，通过对这一系列数据的加权平均来得到t时刻确切的数据。这就是卷积运算.

我们把积分形式离散化就是$s(t)=\sum_{a=-\infty}^{\infty}x(a)w(t-a)$。图像是二维的像素矩阵，因此在图像上做的离散卷积就是$S(i,j)=\sum_m\sum_nI(m,n)Kernel(i-m,j-n)$,因为离散卷积的可交换性，我们可以等价地写作$S(i,j)=\sum_m\sum_nI(i-m,j-n)Kernel(m,n)$。为了方便起见，我们把离散卷积写成表达形式更简单的互相关函数的形式，将这个称为卷积网络的卷积$S(i,j)=\sum_m\sum_nI(i+m,j+n)Kernel(m,n)$

那么，卷积网络有什么性质呢？从公式中我们可以发现，卷积运算的一个好处就是空间不变性，一个卷积核代表一个特征，而在整张图上滑动的卷积运算使得只要这个特征在图中存在，相应的卷积核就能得到激活。多层卷积网络将低维特征的激活值传递到高维，高维融合了更多的特征并对这些特征做加权和，并进行非线性运算，最后根据最终的输出激活向量来进行分类，这就是卷积神经网络的基本思想。而卷积核可以通过先随机初始化，然后通过训练来学习特征。

 

### 前沿方向与问题

现在深度学习在计算机视觉方面广泛用于以下三个领域：

1. Object Detection问题

2. Segmentation问题

   2.1 Semantic Segmentation

   2.2 Instance Segmentation

3. 医学图像处理，主要涉及3D Instance Segmentation问题 

### 著名会议

我总结了一些深度学习方面的顶级会议信息，将其上传到了我的github中，参考[顶级会议整理](https://github.com/FengHZ/Research-Recording-Documents/blob/master/%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A/Top%20Conference%20and%20Journal.pdf)





