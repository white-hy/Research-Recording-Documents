# 关于训练的一些想法

[TOC]

## 梯度爆炸

一个有趣的现象是,当程序从0开始运行到第6个Epoch的第240步的时候，出现了梯度爆炸现象。 注意到我们设置的学习率是0.0001(1e-3)，这个学习率在初始训练的时候起到了很好的结果，在网络运行了3万步后梯度爆炸。

考虑到Adam 算法的特点:

### Adam(Adaptive Moment Estimation)

Adam算法是一个自适应学习率的算法，它为每一个parameter增加自适应调整的学习率。对比Adadelta与RMSprop。Adadelta和RMSprop这两种方法都是采用记录一个指数衰减的梯度平均来实现，具体如下：

1. RMSprop

   假设对于参数$\theta_t$,其通过$Loss$函数所计算的反向梯度为$g_{t}$,给定的初始学习率为$\eta$,那么我们这样计算学习率的衰减：

   $$E[g^{2}]_{t}=0.9*E[g^{2}]_{t-1}+0.1g_{t}^{2}$$ 

   $$\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{E[g^{2}]_{t}+\epsilon}}*g_t$$

   这样我们可以看到，如果我们设置一个中庸的学习率，那么随着一开始梯度很大，那么学习率就会相应减小。但是如果梯度在训练过程中趋于非常小的话，那么我们对于一个t的梯度，上面是一次项，下面是一次项，那么它们比值就会趋向于1，下面再加个$\epsilon$的话，理论上应该是不会爆炸的。

2. Adadelta

   Adadelta的思想是基于Adagrad自动学习学习率思想的。Adagrad需要记录每一个参数所有步长条件下的梯度和，这样对于神经网络的计算量而言是达不到的。因此，Adadelta改进了这个想法，并通过一个巧妙的方法直接避免了设置学习率：

   $$RMS[x]=\sqrt{E[x^2]+\epsilon}$$

   $$\Delta\theta_{t}=-\frac{RMS[\Delta\theta]_{t-1}}{RMS[g]_t}*g_t$$

   $$\theta_{t+1}=\theta_{t}+\Delta\theta_{t}$$

但是这两种方法都记录了冗余的前参数，对于深层网络而言多了很多倍储存量。因此Adam方法应运而生，以不用记录先前的信息以及自适应调节学习率为特点作为主要特色。Adam方法需要输入4个参数，分别是：

$$Initial Learning Rate:(lr),\beta_1(0.9\ usually),\beta_2(0.999\ usually),\epsilon$$

它与*momentum*方法一样，需要记录一个先前的梯度：

$m_{t}=\beta_{1}m_{t-1}+(1-\beta_1)*g_t$

$$v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^2$$

它们初始化都是0,但是作者发现，在起步阶段它们应该通常都不是0，也就是说在初始阶段它们应该对0有很大的偏差，因此这个时候要进行一些修正(尤其是当$\beta_1,\beta_2$非常接近1的时候)，修正为：
$$
\hat{m_{t}}=\frac{m_t}{1-\beta_1^t}=10m_t\\
\hat{v}_t=\frac{v_{t}}{1-\beta_2^t}=100v_t
$$
一般来说，$\beta_1^t=\beta_1,\beta_2^t=\beta_2$

然后参数更新为:
$$
\theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}\hat{m}_t
$$
一般参数配置为$\eta=0.002,\beta_1=0.9,\beta_2=0.999$

那么我们研究一下可以得知，大概$\hat{m}_t$是当前梯度的一个正比函数，10倍可以认为就是1比例。下面也是同样，100倍再开根号sqrt的话也是10倍，这样理论上应该是不会爆炸的。

但是我们观测到的现象是什么呢？是$Loss$函数不断下降的同时训练保持平稳，但是在某一个点处梯度发生了爆炸，那么这就说明，在学习率不变的情况下，出现了梯度爆炸。这是为什么呢？这肯定说明上面的梯度很大，但是下面的梯度很小呗。也就是说，在某一步
$$
\frac{10*(\beta_{1}m_{t-1}+(1-\beta_1)*g_t)}{10*\sqrt{\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^2}}
$$
这一步就爆炸。这个时候理论上前面的梯度应该很小了，那么就是在某一步梯度忽然膨胀了，也就是这个时候梯度出现了某一个部分的爆炸，以下这个值的爆炸？
$$
\frac{g_t}{\sqrt{g_{t}^2}}
$$
我做个数值模拟吧，如何模拟呢？就是说一串基本可以认为是在下降的具有非常大非常显著波动的数值认为是$g_t$,然后我们用公式对$g_t$进行一系列的计算即可。

数值计算模拟失败，一些模拟表示这个应该是一个稳定的东西

所以**所谓吾尝终日而思矣，不如须臾之所学也**,看了两个视频我大概明白了一些，因为我中间用的都是ReLu层，因此神经网络相当于就是
$$
y=w^1*w^2*w^3*...*w^nx=w^1*z^1=w^1*w^2*z^2=...=w1*...*w^{n-1}*z^n
$$
当然其中可能有多条支线，因此求导的结果就是多个矩阵相乘。事实上BN操作就是对输入的一个优化吧，相当于是$w^{k}*(\alpha\frac{input-mean}{std}+\beta)$,等价于$\frac{alpha*w^k}{std}*input-\frac{alpha*mean}{std}+\beta*w^k$

视输入为常数，这样对$w^k$求导就相当于对输入求导同时增加一个约束。但是如果这个约束很大的话就会导致梯度爆炸了。

然后我们假设2次操作，也就是
$$
w^{k-1}*(\alpha_{k}\frac{\frac{\alpha_{k+1}*w^k}{std_{k+1}}*input_{k}-\frac{\alpha_{k+1}*mean_{k+1}}{std_{k+1}}+\beta_{k+1}*w^k-mean_{k}}{std_{k}}+\beta_{k})
$$
对$w^{k-1}$求导，可以得到$\frac{\alpha_k*\alpha_{k+1}}{std_{k}*std_{k+1}}*w^k*input-T$

因此，和$input$有关的项是以多维乘法连起来的，因此这个项的指数增长就会影响到其它的梯度，事实上如果$BN$层所学到的东西加起来进行连乘的话，如果学习的不对那么这个项也会爆炸。这个项的溢出是导致Adam更新不了的主要问题，也就是学习率压不住了。所以调低学习率在这个方面确实是帮了一些忙。不过有个问题是自适应学习率的方法理论上应该不会让梯度爆炸太大吧？难道是学习率一开始就boom了，然后学习率压不下去了？这也不对吧？

为什么会在某个特定的点Boom呢？应该是某个特定点的乘积溢出了吗？这个问题是一个值得解决且必须解决的问题，我觉得可以参考文章[Gradient Explode](https://openreview.net/pdf?id=HkpYwMZRb)来解决。



 



