## Unsupervised Feature Extractor Result

## Result of training:

After 200 step, the lr will decay to 0.99* lr 

After 250 epoch, the lr will decay to 0.9*lr

Then we use kNN to do classification, the result show that our kNN achieve about 82% accuracy just use the unsupervised feature.

核心思想：分类器能够学到类别之间的相似度。

也就是说，我们给出一个测试数据，获得它的feature，然后用与它Feature距离最近(cosine距离)的K个数据进行预测，并用所谓的weight KNN来获得最后结果。这个准确率达到了82%，这说明这个还是很靠谱的。注意到良性与恶性结节比例大概是1：2，因此这个投票并不是一个随机的结果，应该来说它是一个不随机的结果吧，也就是这个确实抓取了图像之间的相似度，然后按照相似度进行了分类。

我们可以把这个结果可视化一下看看。

## Compared with AutoEncoder

我们用之前训练的AutoEncoder来进行kNN的测试，看看结果怎么样。

这就要求我们训练一个kNN，然后看看它们的相似度。

## Compare Cluster Method with just max k-cover

我们试着使用：

DBSCAN

Spectral(recursion)

DBSCAN-Spectral(recrusion)

K-Means

Random Choice

Max k-cover

来选具有代表性的数据，我们看看直接用Max k-cover的效果怎么样（与random choice对比）

## 代码任务

1.先做一个可视化模块，具体而言就是随机选一些图像，计算与它们距离最近的那些图像是什么样的，再保存下来。

分两种：

1.在train数据集中随机选10个[中心crop]，然后找最近的20个图像，把它们存成一个文件夹。同时再找10个最不像的，同样存文件夹

2.在test数据集中随机选10个图像，找出train数据集中最像的几个图像，同时再找10个最不像的，同样存文件夹

2.再对AutoEncoder的特征做一个knn，看看结果怎么样。

3.写一个max k-cover的东西，看看结果怎么样





