# Task 4.26

我采用自动编码器对整个数据集的特征进行编码之后，对它们采用谱聚类的方法进行了聚类。谱聚类后的结果正在训练，但是看样子肯定比随机选要好的多。我随便简单用了用这些聚类算法就得到了这么好的结果，因此这个方法的潜力肯定比我想象中要大。

然后我有一个猜想就是一方面编码特征相似是否就意味着学习的效果比较好呢？另一方面，哪一种距离是衡量编码特征的相似度的最好距离呢？

我做了实验来验证我的猜想。我的实验设计是这样的：

假设我的编码向量是有效的，那么对于我的神经网络而言应该满足以下结果：

如果在test数据集中那些结果非常差的数据所对应的编码向量离我们的原数据的编码向量的这个特征空间比较远，同时结果非常好的数据离我们的特征空间比较近（这里度量远近我用了sin距离和cos距离进行对比），那么我们就可以证明只要离我们的特征空间比较近的数据都可以被我们所拟合，也就是说我们的编码向量确实反应了特征空间的某些信息，也就是说我们的编码向量的选择是正确的。

那么我们的聚类结果就可以说成是把编码向量划分为若干个子空间，然后在每个子空间内都去找那些具有代表性的向量。

我采用了UNet，DBSCAN所寻找出来的400个train数据集与最后的训练结果作为我们的一个尝试实例。我取了400个训练集所对应的这样一个编码特征组成的空间，然后取dice<0.7的数据集作为分类得不好的数据集，取dice>0.7的数据集作为分的好的数据集，在ipynb为4.26上展开实验，结果大致表明这两个数据集确实是具有显著性差异的，分类得好的集合的平均距离要显著地低一些。

但是有一个难题在于，当我取dice>0.85的时候，这些数据反而距离很大，而且与0.7的数据集相比，当我取dice在0.7与0.85之间时，这些数据与特征空间的平均距离远远小于<0.7的分不好的距离。相当于我们的结果出现了一个先向下后向上的这样一个问题。这个问题不解决就很难说我的编码特征是有说服力的。

一个可能可以的解释是此时我的整个train数据集的表现也没有太好，整个train数据集的dice大概在0.82附近，因此此时理论上只有那些在0.7与0.85附近的数据能说明问题。这个仍然要等进一步测试。

>一个可能可以的解释是此时我的整个train数据集的表现也没有太好，整个train数据集的dice大概在0.82附近，因此此时理论上只有那些在0.7与0.85附近的数据能说明问题。这个仍然要等进一步测试。

不对！！！！！注意到我们测试的是整个train数据集，而我们自己的集事实上只是train的一小部分，在这个数据集上我们的参数已经保证了平均的train与test都到了一个极高的数值上。这里是0.93，这说明已经过拟合了。





