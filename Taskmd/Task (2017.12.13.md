# Task Recording

++++++

首先，考虑到loss函数已经那么低了，但是mAP一直上不去，我应该把这些结果拿出来看一看。

因此，今天晚上我的主要任务是远程调用实验室主机，对图片进行预测，然后看一看到底是哪里出了奇奇怪怪的问题。

然后我的策略是这样的：

首先，远程连接并调用机器上已经生成的网络模型，取10个典型的**Train**与**Test**的图片

远程连接的时候取文件为:

`time_1_epoch_4_index_3999_head.pth`

等进行采样，输入之后我们先选10张Test 和 Train的图片，得到它们的ID，然后网络跑一遍。

我们用网络跑的时候所采集的输出结果是这样的：

1.10张图片每一张图片的Loss

2.对10张图片的输出进行over lap的寻找，把那些打了ground truth label的图片给找出来（对Label为真的图片要做bbox regression),将Train 数据的 mAP与Loss拿出来，Test数据的mAP与Loss拿出来进行对比。

3.把这些区域一个一个可视化出来，看其效果。

3个目的都有现成的函数可以调用，可以试着用一下。

第一，计算来自于Test Set的损失为:

`[array([ 2.34043502]), array([ 0.04808307]), array([ 0.02830989]), array([ 42.55413565]), array([ 121.94707289]), array([ 6.99290984]), array([ 0.39284952]), array([ 0.34658829]), array([ 19.49035859]), array([ 3.56119113]), array([ 0.02737078]), array([ 0.14745903]), array([ 44.40588716]), array([ 6.00672989]), array([ 1.56690826]), array([ 19.27115195]), array([ 1.29488071]), array([ 0.18236931]), array([ 0.56079046]), array([ 6.57143532]), array([ 0.58981314]), array([ 85.7309842]), array([ 0.57522436]), array([ 8.63227153]), array([ 53.70528751]), array([ 0.80133601]), array([ 46.83751267]), array([ 108.15874376]), array([ 13.09310384]), array([ 5.14265967]), array([ 36.54743798]), array([ 58.43573999]), array([ 0.40675322]), array([ 38.1924233]), array([ 0.41162049]), array([ 0.02264627]), array([ 204.936997]), array([ 1.19922591]), array([ 0.85566027]), array([ 43.65063582])]`

可以看到，有小有大。

第二，来自于Train Set的损失函数为:

`[array([ 0.01993282]), array([ 0.39143238]), array([ 0.07936098]), array([ 0.09585622]), array([ 0.08522709]), array([ 0.12685175]), array([ 6.25659152]), array([ 0.08280085]), array([ 0.16448919]), array([ 0.05956833]), array([ 0.07427165]), array([ 0.14863663]), array([ 0.79305234]), array([ 0.06547068]), array([ 0.10081792]), array([ 0.13347688]), array([ 0.01510295]), array([ 0.30111654]), array([ 0.03394329]), array([ 0.00261445]), array([ 10.67093827]), array([ 36.85443327]), array([ 0.14858729]), array([ 0.40539921]), array([ 0.03418071]), array([ 60.7545183]), array([ 0.00114001]), array([ 0.41894641]), array([ 0.03986561]), array([ 0.03963731]), array([ 0.34471642]), array([ 13.16181339]), array([ 0.46884292]), array([ 0.30146137]), array([ 0.11892655]), array([ 6.86511967]), array([ 0.3315227]), array([ 0.48007189]), array([ 0.46068146]), array([ 1.59921276])]`

可以看到，训练集上的损失比测试集上的损失要小得多。

然后我们需要看一下在训练集上的mAP：

`,mAP_RPN:0.01342105630934069,mAP_RPN_list:[0.0, 0.0041129350180634655, 0.021452918601028249, 0.011611358750111841, 0.020392859619575196, 0.0, 0.00057865733621792579, 0.021255773255130908, 0.0086390087936984855, 0.020447956155810788, 0.027630436607053464, 0.0052217323828863864, 0.0022950217951474006, 0.0020831611737589491, 0.020744036344349615, 0.017432617147069666, 0.0080282882086116763, 0.0052370096394386912, 0.028809824113549973, 0.020733741700852459, 0.0, 0.015711874854781156, 0.016156814700490178, 0.016619623585675449, 0.0088087343822285495, 0.0047716637251800685, 0.01243762737871924, 0.0092066934696742528, 0.0052851330545616027, 0.037231746359000117, 0.015511962130668532, 0.061085126405607551, 0.011079285461167149, 0.00015310059868305008, 0.039712072941056081, 0.037552421840493302, 0.0019885343589280758, 0.00084884020378956781, 0.017335776457754113, 0.0072888531274442588, 0.032858441017830524, 0.022475234485615427, 0.02788787359049635, 0.00052447564094875994, 0.00071956929147656671, 0.015029996716875299, 0.00072614379811111122, 0.0, 0.005337859247422882, 0.0]`

应该是mAP的算法写错了，或者是我们的loss函数给设计错了

那我应该怎么做呢？

我觉得我需要首先对输出进行分析，即首先我们去可视化一下现在网络的效果

可视化结果出来了。可视化结果显示，我的网络出了一个很大的问题，就是几乎把所有的东西都认为是有object:

$have\_object\_confidence>0.5 $for all bounding box

这是一个非常严重的问题，我因此需要看一下我的损失函数：

我发现问题关键所在了， 问题关键是因为anchor box 根本就生成错了



### Add debug fix

### Discuss my loss function

那么我们的初期采样应该没有什么大的问题了。那唯一的可能就是我的Loss function写错了。

```
Mask cls gt number:5358.0

Object number:168

Mask cls gt number:5678.0

Object number:159

Mask cls gt number:6785.0

Object number:105

Mask cls gt number:5407.0

Object number:188

Mask cls gt number:6049.0

Object number:290

Mask cls gt number:5103.0

Object number:162

Mask cls gt number:5825.0

Object number:83

Mask cls gt number:6208.0

Object number:114

Mask cls gt number:6857.0

Object number:100

Mask cls gt number:5803.0

Object number:72

```

我们可以看到的是，Object number所占的比重总是小的。那么我们为什么倾向于把原图识别为good呢？



有2类问题我还不知道如何处理：

1.就是bbox与分类的loss之间如何平衡的问题。总loss下降但是我觉得bbox训练的并不好，说不定bbox并没有必要加focal loss项



现在mAP是达到了0.29-0.23之间，可以说结果还是处于比较差的阶段。然后我们已经对RPN进行了完善而系统地训练了。



我呢，要对Image做一个筛查，在找RPN的阶段，让我们用好的Image去训练RPN。

因此在训练RPN的阶段我们要做一次筛查，这次筛查的目的