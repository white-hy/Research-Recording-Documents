# 网络总结与尝试

## 网络总结

1. 如果我们直接用预测的数据集进行Test，我们会发现预测的VoE值一直降不下去。比如说直接用整个数据集进行Test，在10k步的时候已经降到了0.46，但是test数据集仍然在0.5附近徘徊。这可能就是一种过拟合。（我可以尝试加入正则化项来考察过拟合的程度）
2. 我发现Batch的数目似乎会非常影响训练的结果。我从4个Batch升到8个Batch之后，Loss下降速度变慢了，但是VoE在Test上连续过了好几轮都不降。

以上两个现象表现出了网络一个非常重要的问题，就是网络自身的学习率，batchsize与过拟合的处理问题。我应该看一下是否过拟合了再做决断

避免过拟合可以试试Dropout，我发现在网络进行到7000k步的时候出现了Train的loss一直下降而Test的loss不下降的情况，中间大概有差了0.1的样子，也就是说这里就出现了过拟合。

## 任务

处理过拟合。

## 结果

Dropout也没法处理过拟合了，或者是Dropout参数设置的不对吧，我先调小Dropout的参数来做这个问题，如何还是不行的话就加入正则化项看看。

如果还是不行的话就只能认为是我们当前的这个网络在数据集上的泛化能力不强，需要加入更多的层数进行训练。

增加L2范数正则化很简单，就是把weight decay增大而已。

或者增加ReLu为PReLu或者Leaky ReLu,这一点非常重要。

## 试着增大几层神经网络

事实证明在DenseNet下，Deep Supervise结构似乎没什么卵用。那就直接暴力增加DenseBlock的数目好了。

## Conclusion

1. PReLu比ReLu泛化更强
2. Dropout的p设置为0.3比较好
3. Parameter Regularization将weight decay 设置为0.01的时候起到了很大的作用

以上三个操作确实降低了过拟合，但是整个网络的收敛与表现更慢了。

我将网络扩展了一个新的Block，正在期待原因

**明天可以尝试数据增广训练，看看是否有更好的效果。**





