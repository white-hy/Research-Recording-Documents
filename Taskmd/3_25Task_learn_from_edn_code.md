# 从efficient densenet code 中学习写作手法

1.训练过程中嵌套3层神经网络进行训练。第一层是对epoch进行训练，而对于每个epoch单独写一个训练函数，

2.可以从torch,utils中找如何进行选择的工具。当然我们也可以随机洗牌再选择。

3.torch.optim.lr_scheduler中有一系列对learning rate进行规划的工具，包括miltistep_lr等工具。

4.我们可以用cvs文件来记录各种损失，并看一下整个文件的过程。

5.我们可以用normalize来进行给定均值和方差的归一化

这些都是好的可以学习的东西。但是最值得学习的还是efficient的代码。

我现已知的是它有全连接，可以进行反向传播（经过测试，反向传播是可以做的。现在我需要修改代码完成卷积操作等）





# 将Growth rate 从6改为8后，一开始下降的非常快，然后后来就开始飞速上升了。似乎从6改为8的效果也并不好。反而是在6的时候效果非常非常不错。难道是因为这个时候反而是Test数据集不行了吗

loss一直在降低，但是Train 与Test的VoE反而在不断增加。

这是为什么呢？

我需要它把图像输出看一看,看看它是不是把所有都预言成了黑色，然后把白色给忽略了。毕竟在最后的gt中白色也很少。这个可能就是Semantic Segmentation中类别不均衡的问题了。

再试一下7个block width与batchsize为4的情况

明天的任务

1.把efficient Densenet给理清学会

2.看一下第10次所记载的参数，把它的test结果输出一下，看一看test结果是不是有问题（与gt相比，是不是全黑，还是全白这种），这个应该是很简单的，但是test result一直不输出我就觉得好奇怪，这段代码的Bug还是要解决一下

3.调整网络，看看有什么更好的结构可以用



