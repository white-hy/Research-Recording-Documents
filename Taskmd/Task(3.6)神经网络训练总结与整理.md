# Task 3.6

+++++

## 训练总结

我发现一个严重但是奇怪的问题，就是我在修改了参数，采用了系统自带的log函数之后训练了2000个epoch最后的test VOE并没有明显下降，甚至连第一次采用`torch.log`函数训练的都不如。这是为什么呢？

对比两次代码

第一次是:

```python
loss = -1 * (gt_label * torch.log(prediction[:, :1, :, :, :]+1e-3) + (1 - gt_label) * torch.log(prediction[:, 1:2, :, :, :])+1e-3)
loss = torch.sum(loss)
```

第二次是：

```python
loss = self._lossfunc(prediction[:, :1, ...], gt_label)
```

这两次很明显的就是第一次多了一个torch.sum。但是我调大学习率之后似乎会出现很大的波动，比如学习率调1e-2就会出现loss大幅度波动到6左右，调1e-3就会快速下降。

这是一个很奇怪的问题，为了解决这个问题我可以做以下3个方面的尝试：

1. Deep Supervise机制是否靠谱

2. 把原函数改成log并用sum进行判断是否比采用BCEloss要靠谱，如果靠谱的话哪里比较靠谱。

   这里可以用Git回退版本来实现

3. 是否神经网络层数不够不能进行完善的拟合

我今天的任务是：

1.等待这次训练完毕，实现上面3个方面的尝试

2.学习神经网络的训练技巧文章，写报告

3.Fluent Python 学习

## 训练总结：

我用

1. 正常训练无Deep Supervise
2. 图片归一化加Deep Supervise
3. 图片归一化无Deep Supervise

这三组样本进行了训练与比较，训练7000步左右，平均时间是60min，误差降到了6e-3附近，但是Test VoE却没有怎么降低。这三组样本细微比较起来是正常训练没有加图片归一化，没有Deep Supervise机制的网络降低得快，而图片归一化加了DeepSupervise的网络则也表现得不坏，但是Test VoE总是有反复，也就是震荡比较大。而图片归一化后没有Deep Supervise的训练则出现了较大的波动，当然这也可能与训练有关。

然后我现在要做的呢，就是把loss函数改一下，改成手动计算log并相加的，看看结果怎么样。

我发现通过这个改动整个神经网络起了质变，这个质变不是指loss上的，但是在8000步的时候loss已经变成了3000，也就是`3000/(64^3)=1e-2`已经达到了这个精度。虽然Loss没有那么高，但是我发现VoE相比之前的降低了不少，而跑出来的predict与gt的对比也比原来好了不少。

我先让它一直跑下去，看看最终能达到一个什么样的结果。

但是这起码给我了一个tips:

**关于交叉熵损失函数的话，一方面要注意torch.log（）自身的数值稳定性问题，另一方面如果用 sum来代替mean的话，说不定会起到很好的效果。**

注意到这并不是学习率的问题。我将学习率由1e-3调到1e-2后就出现了loss的爆炸，似乎学习率能在很大程度上影响到训练的结果。

另外一个可以尝试的事情就是可以用BCELOSS，然后把学习率扩大10^64次方倍？说不定和现在的表现一样了。这是一个可以尝试的思路，先mark上。

可以试着把BCE Loss部分设置为False以获取Sum的loss然后再看情况。

我现在的任务是阅读完训练技巧一文，然后写报告，并总结现在的网络。

我发现使用DenseNet结构之后训练收敛确实很快，效果也不错，浅层网络就可以得到深层的结果。这个结构是不是可以用在我们的suggest系统上呢？应该可以。我们需要再加一些训练迅速的部分然后得出结论。

## 关于反卷积

[反卷积与吸盘效益学习](https://distill.pub/2016/deconv-checkerboard/)

