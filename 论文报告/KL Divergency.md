# KL Divergency

## What is KL Divergency?

Kullback-Leibler divergence is a measure of how one probability distribution is different from a second, reference probability distribution. 

Notice it is a **distribution-wise asymmetric** measure, it can't be used to measure the spread. 

## Definition

$D_{KL}(P||Q)=-\sum_{i}P(i)log(\frac{Q(i)}{P(i)})=\sum_iP(i)log(\frac{P(i)}{Q(i)}))(discrete\ state)$

$D_{KL}(P||Q)=\int_{-\infty}^{\infty}p(x)ln(\frac{p(x)}{q(x)})dx$

if $P==Q$，then$D_{KL}=0$.

$D_{KL}\geq0$(Gibbs inequation)

**gibbs inequation**:

$if \sum_{i}p_i=\sum_iq_{i}=1,p_i,q_i\in(0,1]$,then we have:

$\sum_ip_ilog_2(\frac{p_i}{q_i})\geq 0$.

$proof:$

use that $lnx\leq x-1,\forall x>0(1)$

$\sum_ip_ilog_2(\frac{p_i}{q_i})\geq 0$ is equation to $\sum_ip_ilog_2(\frac{q_i}{p_i})\leq 0$

use (1)

$\sum_i p_i log_2(\frac{q_i}{p_i})\leq \sum_i p_i (\frac{q_i}{p_i}-1)=0$ 

## some characters

$D_{KL}$ is non-negative, not symmetric($D_{KL}(P||Q)\neq D_{KL}(Q||P)$) and can potentially equal infinity.

**Technical Interpretation**:

KL divergence is the "coding penalty" associated with selecting a distribution q to approximate true distribution p

likelihood theory:

the probability that **one observes a set of data given that a particular model** were true.

We have a candidate model for die, the distribution q, and we have many observations $c=\{c_i\}$,which can be formed into a histogram. What is the probability of observing c (appears) if the observation is generated by q? Trivially, we can see that L is proportional to $\Pi_i q_i^{c_i}$,actually, we can see that $L(c|q)=C_n^{c_1}C_{n-c_1}^{c_2}...C_{n-c_1-...-c_{k-1}}^{c_k}q_1^{c_1}...q_k^{c_k}=\frac{n!}{\Pi_i c_i !}\Pi_i q_i^{c_i}$ 

if we grow once, then we get the prob $L(c|q)=q_i$,if we perform more measurements, then maybe L will shrink (because it will multiplicate $(n+1)/(c_j+1)q_j$=1(if $c_i/n \rightarrow q_i$))，maybe it will converge。we use $\bar{L}=L^{\frac{1}{n}}$,$\bar{L}$represent the geometry average possible for one measurement ($L=\bar{L}^n$.then if $\frac{c_i}{n}\rightarrow q_i$，then we can see that :
$$
\bar{L}= (\frac{n!}{\Pi _ic_i !})^{\frac{1}{n}}\Pi_i q_i^{p_i}
$$
$if \frac{c_i}{n}\rightarrow q_i $ then we can see that $L$will not change，L is converged to $\alpha<1$，then $n\rightarrow \infty$。$L\rightarrow 1$but if $\frac{c_i}{n}\rightarrow p_i \neq q_i $,and $p_i$ diverge $q_i$，then $\bar{L}=\Pi_i (\frac{n!}{c_i !})^{\frac{1}{n}}\Pi_i q_i^{p_i}$will decrease to zero(maybe),this time
$$
log_2(\bar{L})=(\frac{1}{n})(log (n!)-\sum _ilog(c_i!))+\sum_ip_ilog(q_i)
$$
$n\rightarrow \infty$,$log(n!)\rightarrow nlogn-n$, and when $n\rightarrow \infty$,for$\frac{c_i}{n}\rightarrow p_i$,then this time$c_i \rightarrow \infty$.
$$
log_2(\bar{L})=(\frac{1}{n})(nlog(n)-n-\sum_i(c_ilog(c_i)-c_i)+\sum_ip_ilog(q_i)\\=log(n)-1-\frac{1}{n}\sum_i(c_ilog(c_i)-c_i)+\sum_ip_ilog(q_i)(1)
$$
Notice :
$$
log(n)=\sum_i(c_i/n)log(n)\\
\sum_i(c_i/n)=1\\
\frac{c_i}{n}\rightarrow p_i
$$


so the equation (1) can be reduced to:
$$
\sum_i[p_ilog(n)-p_ilog(c_i)+p_ilog(q_i)]=\sum_i[-p_ilog(p_i)+p_ilog(q_i)]=-D_{KL}(p||q)
$$
因此我们有$D_{KL}{p||q}=-\sum_ip_ilog(\frac{q_i}{p_i})=\sum_i p_ilog(\frac{p_i}{q_i})=-log_2(\bar{L})(n\rightarrow \infty)$

由上所述，当$p,q$两个分布的概率密度函数几乎处处相等的时候，此时有$\bar{L}=1$，也就是说$D_{KL}=0$，当两个分布相差太大的时候,$\bar{L}\rightarrow 0$,$D_{KL}=\infty$

It bring us an intuition that KL measures if the sampling process $p$ obey our proposal distribution $q$.

## Some usage

### measure identity

We can ask how similar a joint distribution $p(x,y)$  is to the product of its marginals $p(x)p(y)$. The KL divergency can measure the statistical dependence between two variables.

$I(X;Y)=\sum_{x,y}p(x,y)log_2(\frac{p(x,y)}{p(x)p(y)})$

if x,y are statistically independent, then we can see that $I(X;Y)=0$

## Calculate multivariate normal KL divergency

suppose

$P_1(x)=\frac{1}{(2\pi)^{\frac{n}{2}}det(\Sigma_1)^{\frac{1}{2}}}exp(-\frac{1}{2}(x-u_1)^T\Sigma_1^{-1}(x-u_1))$

$P_2(x)=\frac{1}{(2\pi)^{\frac{n}{2}}det(\Sigma_2)^{\frac{1}{2}}}exp(-\frac{1}{2}(x-u_2)^T\Sigma_2^{-1}(x-u_2))$
$$
D(P_1||P_2)=E_{P_1}[log(P_1)-log(P_2)]\\=\frac{1}{2}E_{P_1}[-log(det(\Sigma_1))-(x-u_1)^T\Sigma_1^{-1}(x-u_1)+log(det(\Sigma_2))+(x-u_2)^T\Sigma_2^-1(x-u_2)]\\
\text{drop the common part out}\\
=\frac{1}{2}[log(\frac{det\Sigma_2}{det\Sigma_1})+E_{P_1}[(x-u_2)^T\Sigma_2^{-1}(x-u_2)-(x-u_1)^T\Sigma_1^{-1}(x-u_1)]
$$
$E_{P_1}[(x-u_2)^T\Sigma_2^{-1}(x-u_2)-(x-u_1)^T\Sigma_1^{-1}(x-u_1)]$这里有一个trick:

$(x-u_2)^T\Sigma_2^{-1}(x-u_2)=tr(\Sigma_2^{-1}(x-u_2)(x-u_2)^T)$

 其实就是用了$a^Ta=tr(aa^T)$这个式子

同时注意这里还有第二个trick是在$P_1$的分布下$(x-u_1)(x-u_1)^T=\Sigma_1$,但是$(x-u_2)(x-u_2)^T$则不等于$\Sigma_2$，因为这是在$P_1$的分布下的。
$$
E_{P_1}[(x-u_2)^T\Sigma_2^{-1}(x-u_2)-(x-u_1)^T\Sigma_1^{-1}(x-u_1)]\\=E_{P_1}[tr(\Sigma_2^{-1}(x-u_1+(u_1-u_2))(x-u_1+(u_1-u_2))^T)-1]\\=E_{P_1}[tr(\Sigma_2^{-1}(\Sigma_1+2(x-u_1)(u_1-u_2)^T+(u_1-u_2)(u_1-u_2)^T))]\\=tr(\Sigma_2^{-1}\Sigma_1)+(u_1-u_2)^T\Sigma_2^{-1}(u_1-u_2)-n
$$
带入就有

$D(P_1||P_2)=\frac{1}{2}[log(\frac{det\Sigma_2}{det\Sigma_1})+tr(\Sigma_2^{-1}\Sigma_1)+(u_1-u_2)^T\Sigma_2^{-1}(u_1-u_2)-n]​$  









