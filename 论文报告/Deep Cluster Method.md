# [Unsupervised Deep Embedding for Clustering Analysis](http://proceedings.mlr.press/v48/xieb16.pdf)

[TOC]

## 文章介绍

**Unsupervised Deep Embedding for Clustering Analysis**这篇文章是讲关于如何使用深度学习进行聚类的。这里我只简要指出文章用了哪个方法进行了聚类训练，以及这个方法是如何迭代如何初始化的。

## 文章思想

采用深度学习的聚类方法一般是用神经网络对输入数据进行编码，然后认为最后的编码序列可以代表神经网络的很多特征，然后我们对编码序列进行聚类就可以达成聚类的目的。但是在这一步上有一个问题，就是我们如何进行编码。一般的编码方式有主成分分析等，用深度学习进行编码在聚类上的主要难题是聚类是一个无监督学习的过程，它没有办法反馈信息到我们的深度网络上并让网络对编码进行改善上来。

本文提出了一种比较弱的反馈机制，这种机制使得我们可以对神经网络进行训练，并得到比较好的有效编码。

## 实现手段

文中提出的可以学习的聚类方法是一个类似k-means的迭代聚类方法。

假设在聚类第m步，我们已经有了m步时每一类的重心$centroid_i,i\in{1,...,k}$以及一个编码器，论文给出了以下步骤的训练方案：

1. 用现有编码器进行编码$z_i=f_{\theta}(x_i)$

2. 采用student t分布来作为当前编码点与重心的距离作为$x_i \in Cluster_j$的概率
   $$
   q_{ij}=\frac{(1+||z_i-u_j||^2/\alpha)^{-\frac{\alpha+1}2}}{\sum_{j'}(1+||z_i-u_{j'}||^2)/\alpha)^{-\frac{\alpha+1}2}}
   $$
   通过当前的student t-分布概率来做一个更加紧凑的目标概率，作为我们的target分布，公式如下:
   $$
   p_{ij}=\frac{q_{ij}^2/f_j}{\sum_{j'}q_{ij'}^2/f_{j'}},f_{j}=\sum_iq_{ij}
   $$
   $f_j$可以解释为属于第j类的先验概率，而这里的target分布可以认为是一个对当前预测分布的压缩。

3. 构造目标损失函数如下:
   $$
   L=KL(P||Q)=\sum_i\sum_jp_{ij}log(\frac{p_{ij}}{q_{ij}})
   $$
   这是一个基于KL散度给出的目标损失函数，本质就是让当前聚类更加接近于目标聚类，通过这个函数我们就可以进行反向传播等等。

论文给出了一个初始化方案，首先类别数目我们可以作为一个超参数自己设置，其次作者认为可以先用随机初始化的编码器给每一个点编码，用这些编码并用传统的k-means算法得到初始重心点。

## 如何对图像进行分类

上述其实损失函数是一个比较弱的损失函数，对于图像编码应该力不从心。在图像编码方面，我认为可以用GAN进行预编码，将GAN的输出作为浅层神经网络的输入，用一个浅层神经网络整合GAN的输入，然后用输出作为真正的编码。训练的时候固定GAN的参数，只训练这个浅层编码器作为Fine Turn，然后看看分类结果怎么样。

