# Linear Classification Model

## 生词

analogous 类似

## 模型

输入一个$x$，将其分配到K个不同类之一。

在大部分通常的情况下，类别是互斥的，每一个输入只能被分配到一个类中，也就是说输入空间被分为决定域，它们的边界为决定边界或决定平面。

对于回归问题，目标变量t是由实数组成的向量，我们需要去预测这个。对于那些分类问题，我们需要用目标变量t来表示类别标签。一般用 one-hot编码来表示。比如说我们有5类，那么我们可以有一个目标向量$t=(0,1,0,0,0)^T$来表示。同时我们也可以把这个目标向量t的值解释为目标属于哪一类的概率。

我们可以直接用一个判别式函数来直接把每个向量$x$给归类，我们也可以用条件概率分布进行归类，也就是用$p(C_k|x)$来进行归类。

我们有两种方法来描述$p(C_k|x)$，一种是直接参数化，比如说神经网络模型，输入一个x，把它参数化为$p(x)$，或者我们可以用生成模型来，即采用类条件密度$p(x|C_k)$并融合类先验概率$p(C_k)$，即$p(C_k|x)=\frac{p(x|C_k)p(C_k)}{p(x)}$

在线性回归模型中，模型预测$y(x,w)$是由参数$w$的线性方程给出的。$y(x)=w^Tx+w_0$，因此y是一个实数。对于分类问题，我们想要去预测不同的类别标签，或者说更泛一点，在范围$(0,1)$内的后验概率。为了达到这个目的，我们考虑模型的一个生成，即我们用非线性函数$f(.)$,满足$y(x)=f(w^Tx+w_0)$。

$f(.)$是一个激活函数，在统计学中它的反函数被称作连结函数（在统计学文化中）. 决定平面是一个常数,$y(x)$为常数，因此$w^Tx+w_0$也是常数，因此决定平面为x的线性函数，即使$f(.)$为非线性的（这一句没有理解）。因此，(4.3)所描述的一类模型叫做生成线性模型。但是与用于回归的线性模型不同，因为非线性的存在，参数不再是纯线性的，因此这个问题比线性回归模型具有更复杂的分析与计算特点。

我们先讨论直接在原始的输入空间中进行分类，然后我们将它转换为一个基本函数。

对于2类分类任务，$y(x)=w^Tx+w_0$是很重要的，可以按$y(x)$与0的比较进行分类。因此决定平面为$y(x)=0$.因此假设x是决定平面上的一个点，满足$y(x)=0$，那么正则化距离满足：
$$
\frac{w^Tx}{||w||}=\frac{-w_0}{||w||}
$$
因此我们可以看到偏差参数$w_0$决定了决定平面的位置。随机取一个点x，我们知道它离平面的距离为$\frac{w^Tx+w_0}{||w||}$.

假设我们要分为多类，那么我们就需要建立一个$K-class$的分类器，它可以将多个二分类器进行联合。

假设我们用K-1个分类器，每一个分类器都解决一个二分类问题，这个分类器就是一个$X\ vs \ \bar{X}$的分类器。但是它会导致有一个区域很模糊，不能划分是C1还是C2，就算我们用$K(K-1)/2$个二分类进行划分，我们也不能判断有一个部分的问题，因此我们可以考虑直接划分，比如$y_k(x)=w_k^Tx+w_{k0}$，然后找最大值作为分类。此时$C_k$与$C_j$的边界就是$y_k(x)=y_j(x)$。

用这种方法划分的类决定域应该是连通而且凸的。

假设$\hat{x}=\lambda x_A+(1-\lambda)x_B$，那么我们有$y_k(\hat{x})=\lambda y_k(x_A)+(1-\lambda) y_k(x_B).$因为大于号的传递性，那么$y_k(\hat{x})$与$x_A,x_B$同类。

那么我们到底应该如何学习线性区分函数呢？我们有3个方法，一个是最小权重，一个是Fisher线性判别，还有一个是感知器算法。

## Least squares 

考虑最小二乘损失。每一类$C_k$被它自己的线性模型所描述，满足$y_k(x)=w_k^Tx+w_{k_0}$,$k=1,...,K$.我们用$y(x)=(y_1(x),...,y_n(x))$来进行描述，$y(x)=\hat{W^T}\hat{x}$.假设每一个$x$对应的one-hot编码为$t$，同时定义一个矩阵$T$，注意$E_D(W)$一种期望式写法，写成展开式为

$E_D(W)=\frac{1}{2}\sum_{j=1}^nloss(x_j),loss(x_j)=\sum_{l=1}^K(p_{x_{j},k}-t_{x_{j},k})^2$.

让损失函数的导数为0（损失函数最小，因为损失函数可以没有最大值）

$W=(X^TX)^{-1}X^TT=X^*T$

此时我们可以把损失函数转化为形式$y(x)$.

注意到一个定理，就是这样的损失函数中，如果$t_n$满足$a^Tt_n+b=0$，那么由这个最小二乘法所推导出的$y(x)$就满足$a^Ty(x)+b=0$，这就是概率归一化定理。

（证明）

最小二乘法的问题：

1.对于Outlier缺乏鲁棒性

2.同时那些离surface很远的点对整个方程的影响很大。

这个损失函数对于那些太正确的点的预测具有惩罚作用，影响很大。

为什么最小二乘会失败呢？因为它在高斯模型下与最大似然是等价的。这样那些离群点我们就很好判别。我们可以用其它概率模型来进行分析，现在先用非概率线性分类损失函数模型。

## Fisher 线性区分损失函数

