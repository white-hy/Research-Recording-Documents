# Chap14 自动编码器

## 基本结构

自动编码器就是将输入$x\rightarrow h \rightarrow r$的过程，而中间的编码$h$就是相应的编码。

## 编码器类型

### 欠完备自编码器

最小化一个损失函数$L(x,g(f(x)))$，$L$是一个损失函数，同时$h$的维度小于x

特点：当解码器是线性的，且$L$是均方误差，那么欠完备的自编码会学习出与PCA相同的生成子空间

使用注意：要控制模型容量

### 正则自编码器

注意欠完备自编码器要求模型容量不能太大，而正则编码器在不限制模型容量的时候提出了一些有用的约束使得模型能学习到特征而不限制浅层

#### 稀疏自编码器

$L(x,g(f(x)))+\Omega(h)$

其中我们可以将惩罚项$\Omega(h)$视作加到前馈网络的正则项，这个前馈网络的主要任务是将输入复制到输出并尽量根据这些稀疏特征执行一些监督学习任务。但是这些正则项没有直观的贝叶斯解释，正则编码器不采用这种解释，因为正则项取决于数据。
$$
p_{model}(x,h)=p_{model}(h)p_{model}(x|h)
$$

$$
log\ p_{model}(x) = log \sum_{h}p_{model}(h,x)
$$



因此自编码器可以认为是用一个高似然值的h点来估计整个概率，同时:
$$
log\ p_{model}(h,x)=log p_{model}(h)+log\ p_{model}(x|h)
$$
$log\ p_{model}(h)$我们可以给一个稀疏诱导，比如我们断言$p_{model}(h_i)=\frac{\lambda}{2}e^{-\lambda |h_i|}$

因此通过这个Laplace分布我们可以诱导出：
$$
\Omega(h) = \lambda \sum_i|h_i|
$$

#### 去噪自编码器

去噪自编码器最小化：
$$
L(x,g(f(\hat{x}))
$$
其中$\hat{x}$是被某种噪声损坏的x的副本。因此去噪自编码器可以撤销这些损坏

#### 惩罚项导数作为正则

$$
L(x,g(f(x))) +\Omega(h,x)
$$

但是
$$
\Omega(h,x)=\lambda \sum_i\Vert\nabla_xh_i\Vert^2
$$
它表示对于输入x，x发生了什么巨大的变化并不会影响特征，也就是说这里提取的特征对x是鲁棒的。

### 去噪自编码器

接受损坏数据作为输入，并预测原始未损坏数据作为输出的自编码器

 ### 收缩自编码器

收缩自编码器在编码$h=f(x)$的基础上添加了显式的正则项，使得f的导数尽可能小：
$$
\Omega(h)=\lambda \Vert\partial f(x)/\partial x\Vert_F
$$
也就是说，我们需要让f的导数也尽量小，此时x变得很小那么$f(x)$变化也很小。

收缩自编码器只是在局部收缩，即一个训练样本x的所有扰动都映射到$f(x)$附近。全局来看，两个不同的点$x,x'$会分别被映射到远离原点的两个点$f(x),f(x')$。

### 预测稀疏分解

预测稀疏分解是稀疏编码和参数自编码器的混合模型。参数化自编码器被训练为能够预测迭代推断的输出，而PSD则被应用于图片与视频中对象识别的无监督特征学习。
$$
||x-g(h)||^2 +\lambda|h|_1  +\gamma||h-f(x)||^2
$$
模型由编码器$f(x)$，解码器$g(h)$组成，每次优化的时候我们先固定h优化参数，再固定参数优化h，交替优化。